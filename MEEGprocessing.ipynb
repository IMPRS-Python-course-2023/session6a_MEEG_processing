{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M/EEG processing using MNE-Python\n",
    "\n",
    "*IMPRS - Using Python for Cognitive Science (2023). This tutorial is based on work by [Sophie Slaats](https://www.mpi.nl/people/slaats-sophie), and on  several [MNE-python](https://mne.tools/stable/) tutorials*\n",
    "\n",
    "Good morning! Today we will be working through this tutorial to get familiar with MNE-Python. We are going to do an analysis of MEG/EEG data provided by MNE.\n",
    "\n",
    "At the end of this tutorial you'll know\n",
    "- How to read in M/EEG data\n",
    "- How to visualize the data and detect artefacts \n",
    "- How to perform basic preprocessing steps, such as rereferencing, filtering and (if time permits) identify eyeblinks in the data\n",
    "- How to define Epochs based on trigger values\n",
    "- How to plot ERPs and topomaps \n",
    "\n",
    "\n",
    "If you have any questions, please feel free to ask. Your understanding and feedback are crucial for ensuring a productive and engaging learning experience. So, don't hesitate to share your thoughts or ask for clarification on any part!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start again by making a virtual environment for this week's session. Open a terminal (make sure you're in the virutal environment) and install the necesssary packages using \n",
    "\n",
    "`pip install -r requirements.txt` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing modules\n",
    "\n",
    "We will need os, NumPy, matplotlib, and (*of course*) MNE-Python.\n",
    "\n",
    "<font color='green'>**Exercise 1:**</font> Import them below. We have added matplotlib for you already.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: # this is to display the figures in the notebook\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Your code here\n",
    "\n",
    "\n",
    "%matplotlib inline # this is to display the figures in the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File format\n",
    "MNE-Python uses the FIF file format from Neuromag. The MPI EEG uses BrainVision and the Donders MEG is CTF - that is not a problem, because MNE provides data reader-functions for a wide variety of other data formats (see also here: https://mne.tools/stable/overview/implementation.html#data-formats)\n",
    "\n",
    "Let's now get an example dataset. There are many datasets available on [openneuro](https://openneuro.org/search/modality/eeg), [osf](https://osf.io/) etc. If you want, feel free to get a dataset (one participant) that you find interesting! Note that different datasets are often saved in different ways and formats (which system was used, how triggers were coded etc), so you might run in to some 'problems' later in this tutorial (but we are happy to help with those issues). Otherwise, the code below is generated using sample data from the MNE website. \n",
    "\n",
    "These data were acquired with the Neuromag Vectorview system. EEG data from a 60-channel electrode cap were acquired simultaneously with the MEG (!). In this experiment, checkerboard patterns were presented into the left and right visual field, interspersed by tones to the left or right ear. The interval between the stimuli was 750 ms. Occasionally a smiley face was presented at the center of the visual field. The participant was asked to press a key with the right index finger as soon as possible after the appearance of the smiley face.\n",
    "\n",
    "Trigger codes:\n",
    "\n",
    "    Name     ID\t#Contents\n",
    "    LA\t    1\tResponse to left-ear auditory stimulus\n",
    "    RA\t    2\tResponse to right-ear auditory stimulus\n",
    "    LV\t    3\tResponse to left visual field stimulus\n",
    "    RV\t    4\tResponse to right visual field stimulus\n",
    "    smiley\t5\tResponse to the smiley face\n",
    "    button\t32   Response triggered by the button press\n",
    "    \n",
    "[Source](https://mne.tools/stable/generated/mne.datasets.sample.data_path.html#mne.datasets.sample.data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading in the data\n",
    "\n",
    "To access the data, we use ```read_raw_fif``` function. This creates an instance of MNE's Raw class (= an object!). The Raw object has an attribute <i>info</i>: this attribute contains information about the dataset.\n",
    "\n",
    "Here, we will load the data into memory ```preload``` flag inside the reader function. If we don't do that, the data will be retrieved on the go instead of loading it to memory. This is useful if you're working with multiple participants' data: the memory of your computer is not unlimited.\n",
    "\n",
    "Good to know is that you can still access the info attribute if you don't preload the data.\n",
    "\n",
    "\n",
    "<font color='Purple'>**To do:**</font> Run the code cell below to download the MNE example data. Note that this may take a while. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = mne.datasets.sample.data_path()\n",
    "raw_fname = os.path.join(default_path, 'MEG', 'sample', 'sample_audvis_raw.fif')\n",
    "raw = mne.io.read_raw_fif(raw_fname, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 2:**</font> Print the raw object below to see what information it gives you. Then print the info attribute. What new information do you see? What is the sampling frequency of the data, how many bad channels are there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a particular element of this dictionary, you can use square brackets as you would with a normal Python dictionary.\n",
    "\n",
    "<font color='green'>**Exercise 3:**</font> Try printing the sampling rate, bad channels, and channel names below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of channels: ', raw.info['nchan'])\n",
    "\n",
    "### Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment data can be accessed through raw._data. What shape do you think it should have? How many columns and rows? Try to figure it out using a NumPy function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw objects also have several built-in plotting methods; Here we show the power spectral density (PSD) for each sensor type with `plot_psd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot_psd(fmax=100, picks = 'eeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='green'>**Exercise 4:**</font>  Try to plot 2 seconds of data for 30 channels below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "raw.plot(duration= , n_channels=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot_psd(fmin=40, fmax=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw object also has some other useful methods to manipulate the data. Some of them are:\n",
    "\n",
    "- `raw.copy()` - Get a deep copy of the raw data to avoid modifying the original files\n",
    "- `raw.crop([tmin, tmax])` - Crop the data based on time\n",
    "- `raw.get_data([picks, start, stop,...])` - Get specific parts of the data, without metadata\n",
    "- `raw.filter(l_freq, h_freq[,picks,...])` - Apply band-pass filters\n",
    "- `raw.resample(sfreq[,npad, window, stim_picks, ...])` - Change the sampling rate of the data\n",
    "- `raw.save(fname[,picks, tmin, tmax,...])` - Save data (possibly after preprocessing)\n",
    "- `raw.set_eeg_reference()` - Specify the reference for EEG data\n",
    "- `raw.to_data_frame()` - Export data as a DataFrame to use with MATLAB/R/Pandas\n",
    "` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Filtering data\n",
    "\n",
    "After examining your data, the first thing researchers often do is filter them based on what kind of analysis you want to do. A filter removes or attenuates parts of a signal. Usually, filters act on specific frequency ranges of a signal â€” for example, suppressing all frequency components above or below a certain cutoff value. For an ERP and time-frequency analysis, you would normally want to get rid of very high (e.g., above 100Hz) and very low (e.g., below 0.1 Hz) frequencies. It is important to also remove power line noise (e.g., 50Hz or 60Hz and their harmonics).\n",
    "\n",
    "The values for your low and high pass filters depend on your effect of interest, but cannot exceed the nyquist frequency defined by your sampling rate. That is, it cannot be higher than your sampling rate divided by 2.\n",
    "\n",
    "<font color='purple'>**To think:**</font> What is our sampling rate? And what is the maximum frequency to include in our data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter data with MNE, you first need to select which data are going to be filtered, and subsequently apply the respective filters only on that part of the data (i.e., we don't want to filter the stimulus channel). To accomplish this, we use the MNE function pick_types().\n",
    "\n",
    "The function has the following signature:\n",
    "\n",
    "`mne.pick_types(raw.info, meg=True, eeg=False, stim=False, eog=False, ecg=False, emg=False, ref_meg='auto', misc=False, resp=False, chpi=False, exci=False, ias=False, syst=False, seeg=False, dipole=False, gof=False, bio=False, ecog=False, fnirs=False, include=(), exclude='bads', selection=None)`\n",
    "\n",
    "<font color='green'>**Exercise 5:**</font> In the code cell below, try to use mne.pick_types() to select the EEG channels. Assign them to a variable \"picks\", and print the result. Note: you don't have to specify all of them. 'False' is the default for most channel types, so they will be excluded if you don't specify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "picks = \n",
    "# The following statement returns the IDs of the channels that correspond to this description\n",
    "print(picks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three ways of applying filters to the data. You can either use a method from MNE on the data using mne.filter.filter_data(data, sfreq, picks, l_freq, h_freq) or directly apply the filter to the raw data using raw.filter(picks, l_freq, h_freq) for low/high pass filters, and raw.notch_filter(freqs, picks) to remove power line noise.\n",
    "\n",
    "1. `mne.filter.filter_data(data, sfreq, l_freq, h_freq, picks=None, filter_length='auto', l_trans_bandwidth='auto', h_trans_bandwidth='auto', n_jobs=1, method='fir', iir_params=None, copy=True, phase='zero', fir_window='hamming', fir_design='firwin', pad='reflect_limited', verbose=None)`\n",
    "2. `raw.filter(l_freq, h_freq, picks=None, filter_length='auto', l_trans_bandwidth='auto', h_trans_bandwidth='auto', n_jobs=1, method='fir', iir_params=None, phase='zero', fir_window='hamming', fir_design='firwin', skip_by_annotation=('edge', 'bad_acq_skip'), pad='reflect_limited', verbose=None)`\n",
    "3. `raw.notch_filter(freqs, picks=None, filter_length='auto', notch_widths=None, trans_bandwidth=1.0, n_jobs=1, method='fir', iir_params=None, mt_bandwidth=None, p_value=0.05, phase='zero', fir_window='hamming', fir_design='firwin', pad='reflect_limited', verbose=None)`\n",
    "\n",
    "\n",
    "The difference is that `mne.filter.filter_data()` will apply the filters and return a copy, while the other two methods will apply the filters in-place (directly on the data).\n",
    "\n",
    "By default, MNE applies a FIR filter with Hanning window, but this can be modified to design specific filters. A detailed discussion of filter design is beyond the scope of this tutorial. For more in-depth information on filtering, see [here](https://mne.tools/stable/generated/mne.filter.create_filter.html#mne.filter.create_filter).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 6:**</font> In the code cell below, apply a bandpass filter from 1 to 100 Hz and a notch filter at 60Hz and its first harmonic (120Hz) on our selected channels (\"picks\"). Note: the frequencies in raw.notch_filter are specified by a list of integers (in our case [60, 120]).\n",
    "Then use the plot_psd() method to visualise what you've done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>**To think:**</font> Why would we perform a notch filter at 60 Hz (and its harmonics)?\n",
    "\n",
    "If you want to try different values for the filters, you have to reload the data: you have modified the data loaded in the variable raw, but not the data saved on disk. If you want, you can try it in the block below. Make sure to use a different variable name (not \"raw\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to save your data at this point, you could do this by using: `[your_varname].save('[YOUR_FILENAME].fif')`\n",
    "\n",
    "\n",
    "\n",
    "### 2.2 Artifact removal\n",
    "\n",
    "MNE-Python supports a lot of processing approaches and techniques for artifact correction. Here we'll use Independent Component Analysis (ICA). ICA separates the data into a given number of underlying components. ICA can capture systematic variability caused by physiological processes, such as blinking, breathing, and the heartbeat, in a small number of components. These components have to be identified and removed to clean the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.preprocessing import ICA\n",
    "\n",
    "#in most versions of MNE this worked fine. If you get a scikit-learn error during ICA:\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are different methods for ICA. We will use MNE's default: FastICA. To perform ICA, you have to create an instance of the ICA class, fit it to the data, and then plot the components. To create an ICA instance, use:\n",
    "\n",
    "`ICA(n_components=None, max_pca_components=None, n_pca_components=None, noise_cov=None, random_state=None, method='fastica', fit_params=None, max_iter=200, verbose=None)`\n",
    "\n",
    "Where n_components can be a number of components or a percentage of the data explained, e.g. 25 for 25 components, or 0.95 for 95% of the data explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'fastica'\n",
    "n_components = 25\n",
    "random_state = 23  # this number guarantees that anyone running the script will get the same results\n",
    "\n",
    "ica = ICA(n_components=n_components, method=method, random_state=random_state)\n",
    "print(ica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we have an instance of the ICA class that we can apply to the data. \n",
    "First, for faster processing, we will resample the data. EEG and MEG recordings are notable for their high temporal precision, and are often recorded with sampling rates around 1000 Hz or higher. This is good when precise timing of events is important to the experimental design or analysis plan, but also consumes more memory and computational resources when processing the data.\n",
    "\n",
    "<font color='green'>**Exercise 7:**</font> resample the data to 200Hz below using raw.resample(sfreq). Then apply the ICA to the data using ica.fit(data, picks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# fit ICA \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the topographic maps of the components by using the `ica.plot_components()` method. To plot properties of several components at once and examine them to decide if they are artifacts, you can use `ica.plot_properties(data, [index])`. Try both below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# plot components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# plot properties \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which ones do you think are artifacts, such as blinks, eye movements, or heartbeats?\n",
    "\n",
    "In this example we are going to remove component 0 and 6 as they represent eye movement artifacts (we will not dicsuss EEG artefacts here, for more examples see the MNE webpage). \n",
    "\n",
    "To remove artifacts, we use the method `ica.apply(data, exclude)`. The components are represented by their index in a list, so the second argument must be [0, 6]. In the same way, you can plot how the data would look without the excluded components before removing them by using `ica.plot_overlay(data, exclude)`. Do that first, and then remove the artifacts.\n",
    "\n",
    "<font color='green'>**Exercise 8:**</font> 1) Plot the data without excluded components. 2) apply ICA to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# artefacts_removed = ica.plot_overlay(XXXXX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# raw_ica = ica.apply(XXXXX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental events\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, our data are clean. In order to make averages over trials and further analyze the data, we need to segment the data. To do so, we need to know the onset (and possibly, the offset) of each event as identified by triggers that were sent during the experiment.\n",
    "\n",
    "The function to get events from the data is as follows:\n",
    "\n",
    "```mne.find_events(raw, stim_channel=None, output='onset', consecutive='increasing', min_duration=0, shortest_event=2, mask=None, uint_cast=False, mask_type='and', initial_event=False, verbose=None)```\n",
    "\n",
    "Our stimulus channel (```stim_channel```) is <b>STI 014</b>.\n",
    "\n",
    "<font color='green'>**Exercise 9:**</font> print `raw.ch_names` and find our stimulus channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Detecting events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = mne.find_events(raw_ica, stim_channel='STI 014')\n",
    "\n",
    "# Let's print the first 5 events\n",
    "print(events[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, events have a time stamp (in samples), a code for consecutive events, and an event ID (1, 2, 3 or 4 for the stimuli; 5 for the smiley and 32 for a button press).\n",
    "\n",
    "### 3.2 Epoching\n",
    "\n",
    "To find our epochs, we need to tell the system which codes should be taken into account, and what they mean. For this, we can create a dictionary, containing the relevant events: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for the labels\n",
    "event_ids = {'A/L': 1, 'A/R': 2, 'V/L': 3, 'V/R':4, 'S': 5, 'B': 32}\n",
    "\n",
    "# A/L: left-ear auditory stimulus\n",
    "# A/R: right-ear auditory stimulus\n",
    "# V/L: left visual field stimulus\n",
    "# V/R: right visual field stimulus\n",
    "# S: smiley face\n",
    "# B: button press"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event dictionaries like this one are used when extracting epochs from continuous data; the / character in the dictionary keys allows pooling across conditions by requesting partial condition descriptors (i.e., requesting 'auditory' will select all epochs with Event IDs 1 and 2; requesting 'left' will select all epochs with Event IDs 1 and 3). An example of this is shown in the next section. There is also a convenient plot_events function for visualizing the distribution of events across the duration of the recording (to make sure event detection worked as expected). Here weâ€™ll also make use of the Info attribute to get the sampling frequency of the recording (so our x-axis will be in seconds instead of in samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mne.viz.plot_events(events, event_id=event_ids, sfreq=raw.info['sfreq'],\n",
    "                          first_samp=raw.first_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>**To think:**</font> Does the number of button presses make sense? \n",
    "\n",
    "\n",
    "Next, to segment the data, we create an instance of the Epochs class. This means that, after epoching, the data is no longer in a Raw-object; it is now in an Epochs object, with its own methods and attributes.\n",
    "\n",
    "```mne.Epochs(raw, events, event_id=None, tmin=-0.2, tmax=0.5, baseline=(None, 0), picks=None, preload=False, reject=None, flat=None, proj=True, decim=1, reject_tmin=None, reject_tmax=None, detrend=None, on_missing='error', reject_by_annotation=True, metadata=None, verbose=None)```\n",
    "\n",
    "The parameters for segmenting are the following:\n",
    "\n",
    "* ```tmin``` - Time before trigger\n",
    "* ```tmax``` - Time after trigger\n",
    "* ```preload``` - Used to load the data in memory\n",
    "* ```baseline``` - Baseline corrects each epoch. Normally indicated by using a tuple (None, 0), where None is automatically replaced by tmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = mne.Epochs(raw_ica, events, event_id=event_ids, tmin=-0.2, tmax=0.5, \n",
    "                    picks=picks, preload=True, baseline=(None, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the epoched data, for channel 60: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.plot_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. not very interesting yet. Note that we've just plotted the data across *all trials* and *all channels*. We can now use our dictionary to find, for example, all trials in which a visual stimulus was presented: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_epochs = epochs['V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_epochs.plot_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Now let's have a look at some occipital channels. To figure out the location of the channels, we can plot them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot_sensors(ch_type='eeg', show_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indicate in our plot which channels we want to take into account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_epochs.plot_image(picks=['EEG 058'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After epoching is a good moment to save your data. You can do this in the following way:\n",
    "\n",
    "`epochs.save('sample-epo.fif')` \n",
    "\n",
    "And then read them again:\n",
    "\n",
    "`saved_epochs = mne.read_epochs('sample-epo.fif')`\n",
    "\n",
    "## 4. Evoked responses\n",
    "\n",
    "To average the data by condition you can use the `epochs[condition].average(picks)` method. This will create an Evoked object that can be ploted to visualize the ERP/Fs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_VR = epochs['V/R'].average()\n",
    "VR_plot = evoked_VR.plot(spatial_colors=True, time_unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most effects occur between time point 0 and time point 0.3. Let's make a topographic plot between these time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VR_topoplot = evoked_VR.plot_topomap(times=[0.05, 0.075, 0.1, 0.125, 0.150], time_unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a visual stimulus was presented to the right visual field. As expected, we see activity in the (left) occipital area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='yellow'>**Homework assignment**</font> \n",
    "\n",
    "1. Plot the averaged evoked responses for auditory stimuli, across temporal channels.  \n",
    "2. Try to run a for-loop with all the conditions to visualize the evoked response separately (you can just plot all channels or make a selection, depending on your motivation).\n",
    "3. Plot the averaged evoked response to the smiley faces (catch trials). Does this plot look more noisy? Is this expected? \n",
    "4. Plot the averaged evoked response to 15 V/R trials and visually compare the influence of the number of trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>**Bonus:**</font> Instead of focusing on the EEG, have a look at the MEG data! \n",
    "- What does this tell us about line noise? \n",
    "- Can you detect eye movement components after running ICA? And what about heartbeat effects?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f709d7cb232b41ce914599eec0a9902e5af303746b437c586166f6af9a54b8c7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
